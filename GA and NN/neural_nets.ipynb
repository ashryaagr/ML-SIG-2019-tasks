{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T07:03:32.431974Z",
     "start_time": "2019-06-20T07:03:31.092514Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T07:03:33.360876Z",
     "start_time": "2019-06-20T07:03:33.356535Z"
    }
   },
   "outputs": [],
   "source": [
    "def activation(z, derivative=False):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function:\n",
    "    It handles two modes: normal and derivative mode.\n",
    "    Applies a pointwise operation on vectors\n",
    "\n",
    "    Parameters:\n",
    "    ---\n",
    "    z: pre-activation vector at layer l\n",
    "        shape (n[l], batch_size)\n",
    "    Returns:\n",
    "    pontwize activation on each element of the input z\n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "        return -np.exp(z) / np.square(np.exp(z) + 1)\n",
    "    else:\n",
    "        return 1 / (1 - np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T07:03:36.237262Z",
     "start_time": "2019-06-20T07:03:36.231213Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Mean Square Error between a ground truth vector and a prediction vector\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost: a scalar value representing the loss\n",
    "    \"\"\"\n",
    "    n = y_pred.shape[1]\n",
    "    cost = (1. / (2 * n)) * np.sum((y_true - y_pred)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T07:03:37.201176Z",
     "start_time": "2019-06-20T07:03:37.197816Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost_function_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the loss function w.r.t the activation of the output layer\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost_prime: derivative of the loss w.r.t. the activation of the output\n",
    "    shape: (n[L], batch_size)\n",
    "    \"\"\"\n",
    "    cost_prime = y_pred - y_true \n",
    "    # Calculate the derivative of the cost function\n",
    "    return cost_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:06:11.846384Z",
     "start_time": "2019-06-13T17:06:11.723560Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    This is a custom neural netwok package built from scratch with numpy.\n",
    "    The Neural Network as well as its parameters and training method and procedure will\n",
    "    reside in this class.\n",
    "    Parameters\n",
    "    ---\n",
    "    size: list of number of neurons per layer\n",
    "    Examples\n",
    "    ---\n",
    "    >>> import NeuralNetwork\n",
    "    >>> nn = NeuralNetwork([2, 3, 4, 1])\n",
    "\n",
    "    This means :\n",
    "    1 input layer with 2 neurons\n",
    "    1 hidden layer with 3 neurons\n",
    "    1 hidden layer with 4 neurons\n",
    "    1 output layer with 1 neuron\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, seed=42):\n",
    "        '''\n",
    "        Instantiate the weights and biases of the network\n",
    "        weights and biases are attributes of the NeuralNetwork class\n",
    "        They are updated during the training\n",
    "        '''\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size = size\n",
    "        # biases are initialized randomly\n",
    "        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]\n",
    "\n",
    "        # initialize the weights randomly\n",
    "        \"\"\"\n",
    "        Be careful with the dimensions of the weights\n",
    "        The dimensions of the weight of any particular layer will depend on the\n",
    "        size of the current layer and the previous layer\n",
    "        Example: Size = [16,8,4,2]\n",
    "        The weight file will be a list with 3 matrices with shapes:\n",
    "        (8,16) for weights connecting layers 1 (16) and 2(8)\n",
    "        (4,8) for weights connecting layers 2 (8) and 4(4)\n",
    "        (2,4) for weights connecting layers 3 (4) and 4(2)\n",
    "        Each matrix will be initialized with random values\n",
    "        \"\"\"\n",
    "        self.weights = np.array([np.random.rand(size[a], size[a-1]) for a in range(1, size.size) ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Perform a feed forward computation\n",
    "        Parameters\n",
    "        ---\n",
    "        input: data to be fed to the network with\n",
    "        shape: (input_shape, batch_size)\n",
    "        Returns\n",
    "        ---\n",
    "        a: ouptut activation (output_shape, batch_size)\n",
    "        pre_activations: list of pre-activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        activations: list of activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        '''\n",
    "        a = input\n",
    "        pre_activations = []\n",
    "        activations = [a]\n",
    "        # what does the zip function do?\n",
    "        # It packs up weights and biases. For eg. if weights=[1, 2, 3] and biases=[4,5,6], Then zip(weights, biases) = [[1,4], [2, 5], [3,6]]         \n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = activation(z)\n",
    "            pre_activations.append(z)\n",
    "            activations.append(a)\n",
    "        return a, pre_activations, activations\n",
    "\n",
    "    \"\"\"\n",
    "    Resources:\n",
    "    https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "    https://hmkcode.github.io/ai/backpropagation-step-by-step/\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_deltas(self, pre_activations, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes a list containing the values of delta for each layer using\n",
    "        a recursion\n",
    "        Parameters:\n",
    "        ---\n",
    "        pre_activations: list of of pre-activations. each corresponding to a layer\n",
    "        y_true: ground truth values of the labels\n",
    "        y_pred: prediction values of the labels\n",
    "        Returns:\n",
    "        ---\n",
    "        deltas: a list of deltas per layer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize array to store the derivatives\n",
    "        delta = [0] * (len(self.size) - 1)\n",
    "\n",
    "        # Calculate the delta for each layer\n",
    "        # This is the first step in calculating the derivative\n",
    "        #The last layer is calculated as derivative of cost function *  derivative of sigmoid ( pre-activations of last layer )\n",
    "        delta[-1] = cost_function_prime(y_true, y_pred) * activation(pre_activations[-1], derivative=True)\n",
    "\n",
    "        # Recursively calculate delta for each layer from the previous layer\n",
    "        for l in range(len(deltas) - 2, -1, -1):\n",
    "            # deltas of layer l depend on the weights of layer l and l+1 and on the sigmoid derivative of the pre-activations of layer l\n",
    "            # Note that we use a dot product when multipying the weights and the deltas\n",
    "            # Check their shapes to ensure that their shapes conform to the requiremnts (You may need to transpose some of the matrices)\n",
    "            # The final shape of deltas of layer l must be the same as that of the activations of layer l\n",
    "            # Check if this is true\n",
    "            \n",
    "            delta[l] = np.sum(np.dot(self.weights[l].T, delta[l+1])) * activation(pre_activations[l], True)\n",
    "            # I added np.sum here as the shape of deltas and activations has to be same.\n",
    "            # Why np.sum ? No sure....But according to me any method for reduction of dimension would work (accuracy might vary) \n",
    "        return deltas\n",
    "\n",
    "    def backpropagate(self, deltas, pre_activations, activations):\n",
    "        \"\"\"\n",
    "        Applies back-propagation and computes the gradient of the loss\n",
    "        w.r.t the weights and biases of the network\n",
    "        Parameters:\n",
    "        ---\n",
    "        deltas: list of deltas computed by compute_deltas\n",
    "        pre_activations: a list of pre-activations per layer\n",
    "        activations: a list of activations per layer\n",
    "        Returns:\n",
    "        ---\n",
    "        dW: list of gradients w.r.t. the weight matrices of the network\n",
    "        db: list of gradients w.r.t. the biases (vectors) of the network\n",
    "\n",
    "        \"\"\"\n",
    "        dW = []\n",
    "        db = []\n",
    "        deltas = [0] + deltas\n",
    "        for l in range(1, len(self.size)):\n",
    "            # Compute the derivatives of the weights and the biases from the delta values calculated earlier\n",
    "            # dW_temp depends on the activations of layer l-1 and the deltas of layer l\n",
    "            # dB_temp depends only on the deltas of layer l\n",
    "            # Again be careful of the dimensions and ensure that the dW matrix has the same shape as W\n",
    "            dW_temp = np.dot(activations[l-1], deltas[l] ) \n",
    "            db_temp = deltas[l]\n",
    "            dW.append(dW_temp)\n",
    "            db.append(np.expand_dims(db_temp.mean(axis=1), 1))\n",
    "        return dW, db\n",
    "\n",
    "    def plot_loss(self, epochs, train, test):\n",
    "        \"\"\"\n",
    "        Plots the loss function of the train test data measured every epoch\n",
    "        Parameters:\n",
    "        ---\n",
    "        epochs: number of epochs for training\n",
    "        train: list of losses on the train set measured every epoch\n",
    "        test: list of losses on the test set measured every epoch\n",
    "        \"\"\"\n",
    "\n",
    "        plt.subplot(211)\n",
    "        plt.title('Training Cost (loss)')\n",
    "        plt.plot(range(epochs), train)\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.title('Test Cost (loss)')\n",
    "        plt.plot(range(epochs), test)\n",
    "\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    def train(self,\n",
    "              X,\n",
    "              y,\n",
    "              batch_size,\n",
    "              epochs,\n",
    "              learning_rate,\n",
    "              validation_split=0.2,\n",
    "              print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the network using the gradients computed by back-propagation\n",
    "        Splits the data in train and validation splits\n",
    "        Processes the training data by batches and trains the network using batch gradient descent\n",
    "        Parameters:\n",
    "        ---\n",
    "        X: input data\n",
    "        y: input labels\n",
    "        batch_size: number of data points to process in each batch\n",
    "        epochs: number of epochs for the training\n",
    "        learning_rate: value of the learning rate\n",
    "        validation_split: percentage of the data for validation\n",
    "        print_every: the number of epochs by which the network logs the loss and accuracy metrics for train and validations splits\n",
    "        plot_every: the number of epochs by which the network plots the decision boundary\n",
    "\n",
    "        Returns:\n",
    "        ---\n",
    "        history: dictionary of train and validation metrics per epoch\n",
    "            train_acc: train accuracy\n",
    "            test_acc: validation accuracy\n",
    "            train_loss: train loss\n",
    "            test_loss: validation loss\n",
    "        This history is used to plot the performance of the model\n",
    "        \"\"\"\n",
    "        history_train_losses = []\n",
    "        history_train_accuracies = []\n",
    "        history_test_losses = []\n",
    "        history_test_accuracies = []\n",
    "\n",
    "        # Read about the train_test_split function\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            X.T,\n",
    "            y.T,\n",
    "            test_size=validation_split,\n",
    "        )\n",
    "        x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T\n",
    "\n",
    "        epoch_iterator = range(epochs)\n",
    "\n",
    "        for e in epoch_iterator:\n",
    "            if x_train.shape[1] % batch_size == 0:\n",
    "                n_batches = int(x_train.shape[1] / batch_size)\n",
    "            else:\n",
    "                n_batches = int(x_train.shape[1] / batch_size) - 1\n",
    "\n",
    "            x_train, y_train = shuffle(x_train.T, y_train.T)\n",
    "            x_train, y_train = x_train.T, y_train.T\n",
    "\n",
    "            batches_x = [\n",
    "                x_train[:, batch_size * i:batch_size * (i + 1)]\n",
    "                for i in range(0, n_batches)\n",
    "            ]\n",
    "            batches_y = [\n",
    "                y_train[:, batch_size * i:batch_size * (i + 1)]\n",
    "                for i in range(0, n_batches)\n",
    "            ]\n",
    "\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
    "            db_per_epoch = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for batch_x, batch_y in zip(batches_x, batches_y):\n",
    "                batch_y_pred, pre_activations, activations = self.forward(\n",
    "                    batch_x)\n",
    "                deltas = self.compute_deltas(pre_activations, batch_y,\n",
    "                                             batch_y_pred)\n",
    "                dW, db = self.backpropagate(deltas, pre_activations,\n",
    "                                            activations)\n",
    "                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n",
    "                    dw_per_epoch[i] += dw_i / batch_size\n",
    "                    db_per_epoch[i] += db_i / batch_size\n",
    "\n",
    "                batch_y_train_pred = self.predict(batch_x)\n",
    "\n",
    "                train_loss = cost_function(batch_y, batch_y_train_pred)\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracy = accuracy_score(batch_y.T,\n",
    "                                                batch_y_train_pred.T)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                batch_y_test_pred = self.predict(x_test)\n",
    "\n",
    "                test_loss = cost_function(y_test, batch_y_test_pred)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracy = accuracy_score(y_test.T, batch_y_test_pred.T)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "\n",
    "            # weight update\n",
    "\n",
    "            # What does the enumerate function do?\n",
    "            # It add index to the object that is being enumerated and returns an enumerate object.\n",
    "            \n",
    "            for i, (dw_epoch,\n",
    "                    db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
    "                # Update the weights using the backpropagation algorithm implemented earlier\n",
    "                self.weights[i] = self.weights[i] - learning_rate * dw_epoch\n",
    "                self.biases[i] = self.biases[i] - learning_rate * db_epoch\n",
    "\n",
    "            history_train_losses.append(np.mean(train_losses))\n",
    "            history_train_accuracies.append(np.mean(train_accuracies))\n",
    "\n",
    "            history_test_losses.append(np.mean(test_losses))\n",
    "            history_test_accuracies.append(np.mean(test_accuracies))\n",
    "\n",
    "            if e % print_every == 0:\n",
    "                print(\n",
    "                    'Epoch {} / {} | train loss: {} | train accuracy: {} | val loss : {} | val accuracy : {} '\n",
    "                    .format(e, epochs, np.round(np.mean(train_losses), 3),\n",
    "                            np.round(np.mean(train_accuracies), 3),\n",
    "                            np.round(np.mean(test_losses), 3),\n",
    "                            np.round(np.mean(test_accuracies), 3)))\n",
    "\n",
    "        self.plot_loss(epochs, train_loss, test_loss)\n",
    "\n",
    "        history = {\n",
    "            'epochs': epochs,\n",
    "            'train_loss': history_train_losses,\n",
    "            'train_acc': history_train_accuracies,\n",
    "            'test_loss': history_test_losses,\n",
    "            'test_acc': history_test_accuracies\n",
    "        }\n",
    "        return history\n",
    "\n",
    "    def predict(self, a):\n",
    "        '''\n",
    "        Use the current state of the network to make predictions\n",
    "        Parameters:\n",
    "        ---\n",
    "        a: input data, shape: (input_shape, batch_size)\n",
    "        Returns:\n",
    "        ---\n",
    "        predictions: vector of output predictions\n",
    "        '''\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = activation(z)\n",
    "        predictions = (a > 0.5).astype(int)\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
